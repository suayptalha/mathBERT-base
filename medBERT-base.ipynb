{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom transformers import AdamW\nfrom transformers import BertTokenizer, BertForMaskedLM\nfrom datasets import load_dataset\nimport time\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict=True)\n\ndataset = load_dataset('gayanin/pubmed-gastro-maskfilling')\n\ndata_masked_highlights = dataset['train']['masked_highlights'][:150000]\n\ninputs = tokenizer(\n    data_masked_highlights,\n    max_length=512,\n    truncation=True,\n    padding='max_length',\n    return_tensors='pt'\n)\n\nfor i in range(len(inputs['input_ids'])):\n    for j in range(inputs['input_ids'].shape[1]):\n        if inputs['input_ids'][i, j] == tokenizer.convert_tokens_to_ids('<mask>'):\n            inputs['input_ids'][i, j] = tokenizer.convert_tokens_to_ids('[MASK]')\n\ninputs['labels'] = inputs['input_ids'].clone()\n\nfor i in range(len(inputs['input_ids'])):\n    inputs['labels'][i] = torch.where(inputs['input_ids'][i] == tokenizer.convert_tokens_to_ids('[MASK]'), torch.tensor(-100), inputs['labels'][i])\n\nclass MaskedTextDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, index):\n        input_ids = self.encodings['input_ids'][index]\n        labels = self.encodings['labels'][index]\n        attention_mask = self.encodings['attention_mask'][index]\n        token_type_ids = self.encodings['token_type_ids'][index]\n        return {\n            'input_ids': input_ids,\n            'labels': labels,\n            'attention_mask': attention_mask,\n            'token_type_ids': token_type_ids\n        }\n\ndataset = MaskedTextDataset(inputs)\ndataloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=16,\n    shuffle=True\n)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T11:43:35.515732Z","iopub.execute_input":"2024-12-24T11:43:35.516061Z","iopub.status.idle":"2024-12-24T11:44:55.486246Z","shell.execute_reply.started":"2024-12-24T11:43:35.516038Z","shell.execute_reply":"2024-12-24T11:44:55.485460Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d8b628d374489b8f202c4609129693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bbcb97630d5460e8dd5f654981ffdca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405ef5e6ef43456d9a5dec4a94882088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b0345284ad47e3bfcb42905062c424"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d9798371e5145d49399a7a967a29f44"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"masked_pubmed_highlights_dataset.csv:   0%|          | 0.00/2.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07e4e3dd9d114c3f9e3a322560740593"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/11772 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"778b19d4cc7c49e58b815a617b115a67"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (transform_act_fn): GELUActivation()\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"epochs = 2\noptimizer = AdamW(model.parameters(), lr=5e-5)\nmodel.train()\n\nfor epoch in range(epochs):\n    loop = tqdm(dataloader, dynamic_ncols=True)\n    start_time = time.time()\n    \n    for step, batch in enumerate(loop):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        elapsed_time = time.time() - start_time\n        steps_done = step + 1\n        total_steps = len(loop)\n        remaining_time = (elapsed_time / steps_done) * (total_steps - steps_done)\n        \n        loop.set_description(f\"Epoch {epoch + 1}\")\n        loop.set_postfix(loss=loss.item(), elapsed=f\"{elapsed_time:.2f}s\", remaining=f\"{remaining_time:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T11:45:26.156567Z","iopub.execute_input":"2024-12-24T11:45:26.157063Z","iopub.status.idle":"2024-12-24T12:10:59.981403Z","shell.execute_reply.started":"2024-12-24T11:45:26.157037Z","shell.execute_reply":"2024-12-24T12:10:59.980292Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/736 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ba95576d56499d91b1407073dee9c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/736 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af80c47afbb446998a0cc33b5a4d083a"}},"metadata":{}}],"execution_count":8}]}