{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom transformers import AdamW\nfrom transformers import BertTokenizer, BertForMaskedLM\nfrom datasets import load_dataset\nimport time\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict=True)\n\ndataset = load_dataset('ddrg/named_math_formulas')\n\ndata = dataset['train']['formula'][:150000]\n\ninputs = tokenizer(\n    data,\n    max_length=512,\n    truncation=True,\n    padding='max_length',\n    return_tensors='pt'\n)\n\nrandom_tensor = torch.rand(inputs['input_ids'].shape)\nmasked_tensor = (random_tensor < 0.15)*(inputs['input_ids'] != 101)*(inputs['input_ids'] != 102)*(inputs['input_ids'] != 0)\n\nnonzeros_indices = []\nfor i in range(len(masked_tensor)):\n    nonzeros_indices.append(torch.flatten(masked_tensor[i].nonzero()).tolist())\n\nfor i in range(len(inputs['input_ids'])):\n    inputs['input_ids'][i, nonzeros_indices[i]] = 103\n\ninputs['labels'] = inputs['input_ids'].clone()\nfor i in range(len(inputs['input_ids'])):\n    inputs['labels'][i] = torch.where(masked_tensor[i] == 0, torch.tensor(-100), inputs['labels'][i])\n\nclass MathDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, index):\n        input_ids = self.encodings['input_ids'][index]\n        labels = self.encodings['labels'][index]\n        attention_mask = self.encodings['attention_mask'][index]\n        token_type_ids = self.encodings['token_type_ids'][index]\n        return {\n            'input_ids': input_ids,\n            'labels': labels,\n            'attention_mask': attention_mask,\n            'token_type_ids': token_type_ids\n        }\n\ndataset = MathDataset(inputs)\ndataloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=16,\n    shuffle=True\n)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T08:28:26.555681Z","iopub.execute_input":"2024-12-24T08:28:26.556027Z","iopub.status.idle":"2024-12-24T08:31:21.002117Z","shell.execute_reply.started":"2024-12-24T08:28:26.555999Z","shell.execute_reply":"2024-12-24T08:31:21.001225Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (transform_act_fn): GELUActivation()\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"epochs = 1\noptimizer = AdamW(model.parameters(), lr=5e-5)\nmodel.train()\n\nfor epoch in range(epochs):\n    loop = tqdm(dataloader, dynamic_ncols=True)\n    start_time = time.time()\n    \n    for step, batch in enumerate(loop):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        elapsed_time = time.time() - start_time\n        steps_done = step + 1\n        total_steps = len(loop)\n        remaining_time = (elapsed_time / steps_done) * (total_steps - steps_done)\n        \n        loop.set_description(f\"Epoch {epoch + 1}\")\n        loop.set_postfix(loss=loss.item(), elapsed=f\"{elapsed_time:.2f}s\", remaining=f\"{remaining_time:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T08:33:25.575279Z","iopub.execute_input":"2024-12-24T08:33:25.575599Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9375 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17035351fc946b1a890ccd42b0af054"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
